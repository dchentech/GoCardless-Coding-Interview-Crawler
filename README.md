GoCardless Coding Interview --- David Chen
==================================


Requirements
----------------------------------
We'd like you to write a simple web crawler, in whatever language you're most comfortable in, which given a URL to crawl, should output a site map, showing the static assets for each page. We'd love it if you could also include a README file which explains your design decisions and any parts you found particularly challenging or interesting.

It should be limited to one domain - so when crawling gocardless.com it would crawl all pages within the gocardless.com domain, but not follow the links to our Facebook and Twitter accounts. Weâ€™ll be testing it against gocardless.com.

We will be looking to see if the crawler meets these requirements, and also to see if it meets all and excels at least one of the following areas: robustness, performance, testing, and code structure & layout.


It usually takes around 2 to 3 hours, but don't worry if you feel you want to spend a bit longer on it. Please submit your coding challenge within a week.


The important points from requirements
----------------------------------
* output a site map, showing the static assets for each page
* a README file which explains your design decisions and any parts you found particularly challenging or interesting.
* robustness, performance, testing, and code structure & layout.


Design decisions
----------------------------------
#### Why do this project use Python programming language and Scrapy framework?
1. This is a data project, and Python is very good at data processing.
2. Scrapy is the most famous crawler framework, and it's written in
   Python. The requirements also note that this project should meet
   robustness, performance, testing, code structure & layout. So usually
   the best option is to use a mature framework, instead of reinventing a
   wheel or making a demo job.
3. My **production** programming language experiences include Python, Java,
   Ruby, and JavaScript, so I could choose any of them. It just depends.

#### How to make sure this crawler could find out all of pages which belongs to GoCardless?
First, it's not an easy job, because GoCardless.com is a black box. And just like
other data jobs, we need to pay much more time on the dirty ETL job, and
many exceptional situations.

A first mistake is that if a person has no crawler experience, then who would think
starting crawl from the main page GoCardless.com, and then all pages should
be found. But it's wrong, usually many pages would be missed, because
some pages maybe not be linked in the main page or related pages.

The best entrance is the **sitemap.xml**, which is organized by the website itself.
But as you know, **sitemap.xml** is generated by a program, and a
program usually has bugs, so some pages still would be missed. And other
reasons are "sitemap.xml" would ban some unwanted links, or some
paginated pages, or some need-user-login pages, so some Ajax pages, etc.

The supplement tool should be Google Search "site:gocardless.com". If
the pages are visited by people, then Google would index them, or some
people would share them in public social networks. I tried it, but unfortunately, Google
bans search actions program, e.g. `curl https://www.google.com/\#q\=site:gocardless.com\&start\=30` .
So it still needs some works to do about this.




How to run it?
----------------------------------
Run in a Docker

```bash
./run.sh
```

or local

```bash
./docker-entrypoint.sh
```

And please check the result in `output/result.json` file.


How to run unittest?
----------------------------------
```bash
pip install nose
nosetests
```


Example result data structure
----------------------------------

```json
{
    "https://developer.gocardless.com/2015-07-06/": {
        "css": [
            "stylesheets/all.css",
            "fonts/fonts.css"
        ],
        "image": [
            "images/mandate-setup.png",
            "images/payment-creation.png",
            "images/oauth-authorize.png"
        ],
        "js": [
            "javascripts/all.js",
            "//www.googletagmanager.com/gtm.js?id=GTM-PRFKNC"
        ]
    },
    "https://gocardless.com/": {
    ...
```


Performance report
----------------------------------
| Section       | Value         | Script                                                                          |
|---------------|:-------------:|--------------------------------------------------------------------------------:|
| Page count    | 604           | ruby -e 'require "json"; puts JSON.parse(File.read("output/result.json")).size' |
| Time cost     | 25.6 seconds  | time ./run.sh                                                                   |


TODO
----------------------------------
* extract image/assets from https://gocardless.com/bundle/main-22d1be7d280524ff318e.css
